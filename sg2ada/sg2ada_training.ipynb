{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPI5E5y0pujD"
   },
   "source": [
    "# Custom Training StyleGan2-ADA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SI_i1MwgpzOD"
   },
   "source": [
    "This notebook is made to work with Jupyter Notebooks on a dedicated server. If you would like to train StyleGAN2 in Colab please use [Derrick Schultz's original notebook](https://github.com/dvschultz/ai/blob/master/StyleGAN2.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StyleGAN2-ADA only work with Tensorflow 1. Run the next cell before anything else to make sure we’re using TF1 and not TF2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iKYAU7Wub3WW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 3291454608227213190)\n",
      "_DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14540349452324919188)\n",
      "_DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 7971792487, 5772944377220684582)\n",
      "_DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 13877897758306955812)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  devices = sess.list_devices()\n",
    "  for device in devices:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "51ei6d5kxVDm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 15 22:54:17 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   30C    P8     6W / 180W |     11MiB /  8119MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1117      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A      1858      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YcUMPQp6ipP"
   },
   "source": [
    "## Clone Repo locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epV6TDzAjox1"
   },
   "source": [
    "Run this cell. If you’re already installed the repo, it will skip the installation process and change into the repo’s directory. If you haven’t installed it, it will install all the files necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8HX77jscX2zV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isdir(\"stylegan2-ada\"):\n",
    "    %cd \"stylegan2-ada\"\n",
    "else:\n",
    "    #install script\n",
    "    !git clone https://github.com/dvschultz/stylegan2-ada\n",
    "    %cd stylegan2-ada\n",
    "    !mkdir downloads\n",
    "    !mkdir datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config user.name \"test\"\n",
    "!git config user.email \"test@test.com\"\n",
    "!git fetch origin\n",
    "!git checkout origin/main -- train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "\n",
    "Remember that the dataset has to be preprocessed and __resize at the correct size (in our case 1024x1024)__. You can use this handy library to do that: https://github.com/dvschultz/dataset-tools\n",
    "\n",
    "Upload your dataset as a zip file to Google drive and note the file ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Hv0wiw5K5L4LxDAPqMsESm2jLeav5NYD\n",
      "To: /home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/flowers-sq-1024.zip\n",
      "2.91GB [10:44, 4.51MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'flowers-sq-1024.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# file_id = 'YOUR-FILE-ID'\n",
    "file_id = '1_eOre42oqWaMfx8riyAE-p5IFxi3EZRB'\n",
    "\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "gdown.download(url, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeS9tDvt61VG"
   },
   "source": [
    "## Convert dataset to .tfrecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Q58MJbckLUc"
   },
   "source": [
    "**Note: You only need to do this once per dataset. If you have already run this and are returning to continue training, skip these cells.**\n",
    "\n",
    "We need to convert our image dataset to a format that StyleGAN2-ADA can read from.\n",
    "\n",
    "First we unzip our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8JUP51nJdEjz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting flowers-sq-1024.zip ...\n",
      "patool: ... flowers-sq-1024.zip extracted to `dataset_imgs'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dataset_imgs'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import patoolib\n",
    "\n",
    "# zip_path = \"YOUR-ZIP-FILE.zip\"\n",
    "zip_path = \"flowers-sq-1024-small.zip\"\n",
    "\n",
    "!mkdir dataset_imgs\n",
    "patoolib.extract_archive(zip_path, outdir='dataset_imgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0QH0nzjlbEE"
   },
   "source": [
    "Now that your image dataset is ready, we need to convert it to the `.tfrecords` format.\n",
    "\n",
    "Depending on the resolution of your images and how many you have, this can take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T-BZHhBe7AvO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from \"dataset_imgs/flowers-sq-1024\"\n",
      "Creating dataset \"./datasets/flowers-sq-1024\"\n",
      "Added 40 images.\n"
     ]
    }
   ],
   "source": [
    "#update this to the path to your image folder\n",
    "dataset_path = \"dataset_imgs/flowers-sq-1024-small\"\n",
    "#give your dataset a name\n",
    "dataset_name = \"flowers-sq-1024-small\"\n",
    "\n",
    "#you don't need to edit anything here\n",
    "!python dataset_tool.py create_from_images ./datasets/{dataset_name} {dataset_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DvTupHzP2s_"
   },
   "source": [
    "## Train a custom model\n",
    "\n",
    "We’re ready to start training! There are numerous arguments to training, what’s listed below are the most popular options. To see all the options, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Fxu7CA0Qb1Yd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] --outdir DIR [--gpus INT] [--snap INT] [--seed INT] [-n]\n",
      "                --data PATH [--res INT] [--mirror BOOL] [--metrics LIST]\n",
      "                [--metricdata PATH]\n",
      "                [--cfg {auto,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline}]\n",
      "                [--gamma FLOAT] [--kimg INT] [--aug {noaug,ada,fixed,adarv}]\n",
      "                [--p FLOAT] [--target TARGET]\n",
      "                [--augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}]\n",
      "                [--cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}]\n",
      "                [--dcap FLOAT] [--resume RESUME] [--freezed INT]\n",
      "\n",
      "Train a GAN using the techniques described in the paper\n",
      "\"Training Generative Adversarial Networks with Limited Data\".\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "general options:\n",
      "  --outdir DIR          Where to save the results (required)\n",
      "  --gpus INT            Number of GPUs to use (default: 1 gpu)\n",
      "  --snap INT            Snapshot interval (default: 50 ticks)\n",
      "  --seed INT            Random seed (default: 1000)\n",
      "  -n, --dry-run         Print training options and exit\n",
      "\n",
      "training dataset:\n",
      "  --data PATH           Training dataset path (required)\n",
      "  --res INT             Dataset resolution (default: highest available)\n",
      "  --mirror BOOL         Augment dataset with x-flips (default: false)\n",
      "\n",
      "metrics:\n",
      "  --metrics LIST        Comma-separated list or \"none\" (default: fid50k_full)\n",
      "  --metricdata PATH     Dataset to evaluate metrics against (optional)\n",
      "\n",
      "base config:\n",
      "  --cfg {auto,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline}\n",
      "                        Base config (default: auto)\n",
      "  --gamma FLOAT         Override R1 gamma\n",
      "  --kimg INT            Override training duration\n",
      "\n",
      "discriminator augmentation:\n",
      "  --aug {noaug,ada,fixed,adarv}\n",
      "                        Augmentation mode (default: ada)\n",
      "  --p FLOAT             Specify augmentation probability for --aug=fixed\n",
      "  --target TARGET       Override ADA target for --aug=ada and --aug=adarv\n",
      "  --augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}\n",
      "                        Augmentation pipeline (default: bgc)\n",
      "\n",
      "comparison methods:\n",
      "  --cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}\n",
      "                        Comparison method (default: nocmethod)\n",
      "  --dcap FLOAT          Multiplier for discriminator capacity\n",
      "\n",
      "transfer learning:\n",
      "  --resume RESUME       Resume from network pickle (default: noresume)\n",
      "  --freezed INT         Freeze-D (default: 0 discriminator layers)\n",
      "\n",
      "examples:\n",
      "\n",
      "  # Train custom dataset using 1 GPU.\n",
      "  python train.py --outdir=~/training-runs --gpus=1 --data=~/datasets/custom\n",
      "\n",
      "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
      "  python train.py --outdir=~/training-runs --gpus=2 --data=~/datasets/cifar10c \\\n",
      "      --cfg=cifar\n",
      "\n",
      "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
      "  python train.py --outdir=~/training-runs --gpus=4 --data=~/datasets/metfaces \\\n",
      "      --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
      "\n",
      "  # Reproduce original StyleGAN2 config F.\n",
      "  python train.py --outdir=~/training-runs --gpus=8 --data=~/datasets/ffhq \\\n",
      "      --cfg=stylegan2 --res=1024 --mirror=1 --aug=noaug\n",
      "\n",
      "available base configs (--cfg):\n",
      "  auto           Automatically select reasonable defaults based on resolution\n",
      "                 and GPU count. Good starting point for new datasets.\n",
      "  stylegan2      Reproduce results for StyleGAN2 config F at 1024x1024.\n",
      "  paper256       Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
      "  paper512       Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
      "  paper1024      Reproduce results for MetFaces at 1024x1024.\n",
      "  cifar          Reproduce results for CIFAR-10 (tuned configuration).\n",
      "  cifarbaseline  Reproduce results for CIFAR-10 (baseline configuration).\n",
      "\n",
      "transfer learning source networks (--resume):\n",
      "  ffhq256        FFHQ trained at 256x256 resolution.\n",
      "  ffhq512        FFHQ trained at 512x512 resolution.\n",
      "  ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
      "  celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
      "  lsundog256     LSUN Dog trained at 256x256 resolution.\n",
      "  <path or URL>  Custom network pickle.\n"
     ]
    }
   ],
   "source": [
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jOftFoyiDU3s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training options:\n",
      "{\n",
      "  \"G_args\": {\n",
      "    \"func_name\": \"training.networks.G_main\",\n",
      "    \"fmap_base\": 16384,\n",
      "    \"fmap_max\": 512,\n",
      "    \"mapping_layers\": 8,\n",
      "    \"num_fp16_res\": 4,\n",
      "    \"conv_clamp\": 256\n",
      "  },\n",
      "  \"D_args\": {\n",
      "    \"func_name\": \"training.networks.D_main\",\n",
      "    \"mbstd_group_size\": 4,\n",
      "    \"fmap_base\": 16384,\n",
      "    \"fmap_max\": 512,\n",
      "    \"num_fp16_res\": 4,\n",
      "    \"conv_clamp\": 256\n",
      "  },\n",
      "  \"G_opt_args\": {\n",
      "    \"beta1\": 0.0,\n",
      "    \"beta2\": 0.99,\n",
      "    \"learning_rate\": 0.002\n",
      "  },\n",
      "  \"D_opt_args\": {\n",
      "    \"beta1\": 0.0,\n",
      "    \"beta2\": 0.99,\n",
      "    \"learning_rate\": 0.002\n",
      "  },\n",
      "  \"loss_args\": {\n",
      "    \"func_name\": \"training.loss.stylegan2\",\n",
      "    \"r1_gamma\": 10\n",
      "  },\n",
      "  \"augment_args\": {\n",
      "    \"class_name\": \"training.augment.AdaptiveAugment\",\n",
      "    \"tune_heuristic\": \"rt\",\n",
      "    \"tune_target\": 0.6,\n",
      "    \"apply_func\": \"training.augment.augment_pipeline\",\n",
      "    \"apply_args\": {\n",
      "      \"xflip\": 1,\n",
      "      \"rotate90\": 1,\n",
      "      \"xint\": 1,\n",
      "      \"scale\": 1,\n",
      "      \"rotate\": 1,\n",
      "      \"aniso\": 1,\n",
      "      \"xfrac\": 1\n",
      "    },\n",
      "    \"tune_kimg\": 100\n",
      "  },\n",
      "  \"num_gpus\": 1,\n",
      "  \"image_snapshot_ticks\": 4,\n",
      "  \"network_snapshot_ticks\": 4,\n",
      "  \"train_dataset_args\": {\n",
      "    \"path\": \"./datasets/flowers-sq-1024\",\n",
      "    \"max_label_size\": 0,\n",
      "    \"resolution\": 1024,\n",
      "    \"mirror_augment\": true\n",
      "  },\n",
      "  \"metric_arg_list\": [],\n",
      "  \"metric_dataset_args\": {\n",
      "    \"path\": \"./datasets/flowers-sq-1024\",\n",
      "    \"max_label_size\": 0,\n",
      "    \"resolution\": 1024,\n",
      "    \"mirror_augment\": true\n",
      "  },\n",
      "  \"total_kimg\": 25000,\n",
      "  \"minibatch_size\": 32,\n",
      "  \"minibatch_gpu\": 4,\n",
      "  \"G_smoothing_kimg\": 10,\n",
      "  \"G_smoothing_rampup\": null,\n",
      "  \"resume_pkl\": \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl\",\n",
      "  \"run_dir\": \"./results/00000-flowers-sq-1024-mirror-stylegan2-bg-resumeffhq1024\"\n",
      "}\n",
      "\n",
      "Output directory:  ./results/00000-flowers-sq-1024-mirror-stylegan2-bg-resumeffhq1024\n",
      "Training data:     ./datasets/flowers-sq-1024\n",
      "Training length:   25000 kimg\n",
      "Resolution:        1024\n",
      "Number of GPUs:    1\n",
      "\n",
      "Creating output directory...\n",
      "Loading training set...\n",
      "Image shape: [3, 1024, 1024]\n",
      "Label shape: [0]\n",
      "\n",
      "Constructing networks...\n",
      "Setting up TensorFlow plugin \"fused_bias_act.cu\": Compiling... Failed!\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 561, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 553, in main\n",
      "    run_training(**vars(args))\n",
      "  File \"train.py\", line 451, in run_training\n",
      "    training_loop.training_loop(**training_options)\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/training/training_loop.py\", line 123, in training_loop\n",
      "    Gs = G.clone('Gs')\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/network.py\", line 457, in clone\n",
      "    net.copy_vars_from(self)\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/network.py\", line 490, in copy_vars_from\n",
      "    src_net._get_vars()\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/network.py\", line 297, in _get_vars\n",
      "    self._vars = OrderedDict(self._get_own_vars())\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/network.py\", line 286, in _get_own_vars\n",
      "    self._init_graph()\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/network.py\", line 151, in _init_graph\n",
      "    out_expr = self._build_func(*self._input_templates, **build_kwargs)\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/training/networks.py\", line 231, in G_main\n",
      "    num_layers = components.synthesis.input_shape[1]\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/network.py\", line 232, in input_shape\n",
      "    return self.input_shapes[0]\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/network.py\", line 219, in input_shapes\n",
      "    self._input_shapes = [t.shape.as_list() for t in self.input_templates]\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/network.py\", line 267, in input_templates\n",
      "    self._init_graph()\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/network.py\", line 151, in _init_graph\n",
      "    out_expr = self._build_func(*self._input_templates, **build_kwargs)\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/training/networks.py\", line 439, in G_synthesis\n",
      "    x = layer(x, layer_idx=0, fmaps=nf(1), kernel=3)\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/training/networks.py\", line 392, in layer\n",
      "    x = modulated_conv2d_layer(x, dlatents_in[:, layer_idx], fmaps=fmaps, kernel=kernel, up=up, resample_kernel=resample_kernel, fused_modconv=fused_modconv)\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/training/networks.py\", line 105, in modulated_conv2d_layer\n",
      "    s = apply_bias_act(s, bias_var='mod_bias', trainable=trainable) + 1 # [BI] Add bias (initially 1).\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/training/networks.py\", line 50, in apply_bias_act\n",
      "    return fused_bias_act(x, b=tf.cast(b, x.dtype), act=act, gain=gain, clamp=clamp)\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/ops/fused_bias_act.py\", line 72, in fused_bias_act\n",
      "    return impl_dict[impl](x=x, b=b, axis=axis, act=act, alpha=alpha, gain=gain, clamp=clamp)\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/ops/fused_bias_act.py\", line 132, in _fused_bias_act_cuda\n",
      "    cuda_op = _get_plugin().fused_bias_act\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/ops/fused_bias_act.py\", line 18, in _get_plugin\n",
      "    return custom_ops.get_plugin(os.path.splitext(__file__)[0] + '.cu')\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/custom_ops.py\", line 159, in get_plugin\n",
      "    _run_cmd(nvcc_cmd + ' \"%s\" --shared -o \"%s\" --keep --keep-dir \"%s\"' % (cuda_file, tmp_file, tmp_dir))\n",
      "  File \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/custom_ops.py\", line 69, in _run_cmd\n",
      "    raise RuntimeError('NVCC returned an error. See below for full command line and output log:\\n\\n%s\\n\\n%s' % (cmd, output))\n",
      "RuntimeError: NVCC returned an error. See below for full command line and output log:\n",
      "\n",
      "nvcc --compiler-options '-fPIC' --compiler-options '-I/home/guma/miniconda3/envs/stylegan2-ada/lib/python3.7/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=1' --linker-options '-L/home/guma/miniconda3/envs/stylegan2-ada/lib/python3.7/site-packages/tensorflow -l:libtensorflow_framework.so.1' --gpu-architecture=sm_61 --use_fast_math --disable-warnings --include-path \"/home/guma/miniconda3/envs/stylegan2-ada/lib/python3.7/site-packages/tensorflow/include\" --include-path \"/home/guma/miniconda3/envs/stylegan2-ada/lib/python3.7/site-packages/tensorflow/include/external/protobuf_archive/src\" --include-path \"/home/guma/miniconda3/envs/stylegan2-ada/lib/python3.7/site-packages/tensorflow/include/external/com_google_absl\" --include-path \"/home/guma/miniconda3/envs/stylegan2-ada/lib/python3.7/site-packages/tensorflow/include/external/eigen_archive\" 2>&1 \"/home/guma/Documents/App/ML/datacrunch-stylegan2-ada/stylegan2-ada/dnnlib/tflib/ops/fused_bias_act.cu\" --shared -o \"/tmp/tmptz7oprbr/fused_bias_act_tmp.so\" --keep --keep-dir \"/tmp/tmptz7oprbr\"\n",
      "\n",
      "/bin/sh: 1: nvcc: not found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this name must EXACTLY match the dataset name you used when creating the .tfrecords file\n",
    "dataset_name = \"flowers-sq-1024-small\"\n",
    "#how often should the model generate samples and a .pkl file\n",
    "snapshot_count = 4\n",
    "#should the images be mirrored left to right?\n",
    "mirrored = True\n",
    "#should the images be mirrored top to bottom?\n",
    "mirroredY = False\n",
    "#metrics? \n",
    "metric_list = None\n",
    "#augments\n",
    "augs = \"bg\"\n",
    "\n",
    "#\n",
    "# this is the most important cell to update\n",
    "#\n",
    "# running it for the first time? set it to ffhq(+resolution)\n",
    "# resuming? get the path to your latest .pkl file and use that\n",
    "# resume_from = \"/content/drive/My\\ Drive/colab-sg2-ada2/stylegan2-ada/results/00008-dante1024-mirror-mirrory-11gb-gpu-bg-resumecustom/network-snapshot-000160.pkl\"\n",
    "resume_from = \"ffhq1024\"\n",
    "\n",
    "#don't edit this unless you know what you're doing :)\n",
    "!python train.py --outdir ./results --snap={snapshot_count} --cfg=11gb-gpu --data=./datasets/{dataset_name} --augpipe={augs} --mirror={mirrored} --mirrory={mirroredY} --metrics={metric_list} --resume={resume_from}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lohotw1FqC54"
   },
   "source": [
    "### While it’s training...\n",
    "**Once the above cell is running you should be training!**\n",
    "\n",
    "Don’t close this tab! Colab needs to be open and running in order to continue training. Every ~15min or so a new line should get added to your output, indicated its still training. Depending on you `snapshot_count` setting you should see the results folder in your Google drive folder fill with both samples (`fakesXXXXXx.jpg`) and model weights (`network-snapshot-XXXXXX.pkl`). The samples are worth looking at while it trains but don’t get too worried about each individual sample.\n",
    "\n",
    "If you chose a metric, you will also see scores for each snapshot. Don’t obsess over these! they are a guide, it can go up or down slightly for each snapshot. What you want to see is a gradual lowering of the score over time.\n",
    "\n",
    "Once Colab shuts off, you can Reconnect the notebook and re-run every cell from top to bottom. Make sure you update the `resume_from` path to continue training from the latest model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of Stylegan2-ada Custom Training.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:stylegan2-ada]",
   "language": "python",
   "name": "conda-env-stylegan2-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
