{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3dd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f3751",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make sure we are in the right env\n",
    "!conda info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e557fd7",
   "metadata": {},
   "source": [
    "# Only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63565fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch\n",
    "%cd stylegan2-ada-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/openai/CLIP CLIP\n",
    "!mv CLIP/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
    "print(\"CUDA version:\", CUDA_version)\n",
    "\n",
    "if CUDA_version == \"10.0\":\n",
    "    torch_version_suffix = \"+cu100\"\n",
    "elif CUDA_version == \"10.1\":\n",
    "    torch_version_suffix = \"+cu101\"\n",
    "elif CUDA_version == \"10.2\":\n",
    "    torch_version_suffix = \"\"\n",
    "else:\n",
    "    torch_version_suffix = \"+cu110\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a170549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed the first time\n",
    "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install click requests tqdm pyspng ninja gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!DNNLIB_CACHE_DIR=.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a45c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gdown --id 1CNhPQH3cuDJSOX2RE5PDkFkD56WkC_u9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac0b7f",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd stylegan2-ada-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import PIL.Image\n",
    "import torch.nn.functional as F\n",
    "import clip\n",
    "import dnnlib\n",
    "import legacy\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_avg_samples = 65536 # total samples to cluster\n",
    "num_categories = 64   # total number of top level clusters\n",
    "batch_size = 16\n",
    "network_pkl = 'network-snapshot-000088.pkl'\n",
    "prefix = \"flowers\"\n",
    "suffix = \"64k\"\n",
    "device = torch.device('cuda')\n",
    "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(device)\n",
    "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ae48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(model_file, 'rb') as f:\n",
    "#    G = pickle.load(f)['G_ema'].requires_grad_(False).to(device)  # torch.nn.Module\n",
    "with dnnlib.util.open_url(network_pkl) as fp:\n",
    "    G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples in W\n",
    "z_samples = np.random.randn(w_avg_samples, G.z_dim)\n",
    "labels = None\n",
    "if (G.mapping.c_dim):\n",
    "    labels = torch.from_numpy(0.2*np.random.randn(w_avg_samples, G.mapping.c_dim)).to(device)\n",
    "w_samples = G.mapping(torch.from_numpy(z_samples).to(device), labels)  # [N, L, C]\n",
    "w_samples = w_samples.cpu().numpy().astype(np.float32)                 # [N, L, C]\n",
    "w_samples_1d = w_samples[:, :1, :].astype(np.float32).squeeze()\n",
    "print(w_samples_1d.shape)\n",
    "np.save(prefix + \"_latents_\" + suffix + \".npy\", w_samples_1d)\n",
    "w = w_samples_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "model2, _ = clip.load(\"RN50x4\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326054de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images to get CLIP image features (two models used); could also get other perceptual model features in this step, such as LPIPS\n",
    "# This step is certainly the slowest, will be faster with a lower number of total samples, or with less models being run.\n",
    "logits1 = []\n",
    "logits2 = []\n",
    "latents = w\n",
    "for i in tqdm(range(latents.shape[0]//batch_size)):\n",
    "    images = G.synthesis(torch.tensor(np.tile(np.expand_dims(latents[i*batch_size:(i+1)*batch_size,:],axis=1),[1,G.mapping.num_ws,1]), dtype=torch.float32, device=device), noise_mode='const')\n",
    "    with torch.no_grad():\n",
    "        image_input = (torch.clamp(images, -1, 1) + 1) * 0.5\n",
    "        image_input = F.interpolate(image_input, size=(256, 256), mode='area')\n",
    "        image_input = image_input[:, :, 16:240, 16:240] # 256 -> 224, center crop\n",
    "        image_input -= image_mean[None, :, None, None]\n",
    "        image_input /= image_std[None, :, None, None]\n",
    "        image_features = model1.encode_image(image_input)\n",
    "        logits1.append(image_features.cpu().numpy())\n",
    "\n",
    "        image_input = (torch.clamp(images, -1, 1) + 1) * 0.5\n",
    "        image_input = F.interpolate(image_input, size=(324, 324), mode='area')\n",
    "        image_input = image_input[:, :, 18:306, 18:306] # 324 -> 288, center crop\n",
    "        image_input -= image_mean[None, :, None, None]\n",
    "        image_input /= image_std[None, :, None, None]\n",
    "        image_features = model2.encode_image(image_input)\n",
    "        logits2.append(image_features.cpu().numpy())\n",
    "\n",
    "logits1 = np.array(logits1)\n",
    "logits1 = logits1.reshape(-1, *logits1.shape[2:]).squeeze()\n",
    "print(logits1.shape)\n",
    "clip1 = logits1\n",
    "np.save(prefix + \"_clip_\" + suffix + \"_1.npy\", clip1)\n",
    "\n",
    "logits2 = np.array(logits2)\n",
    "logits2 = logits2.reshape(-1, *logits2.shape[2:]).squeeze()\n",
    "print(logits2.shape)\n",
    "clip2 = logits2\n",
    "np.save(prefix + \"_clip_\" + suffix + \"_2.npy\", clip2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9505b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stylegan2-ada-pytorch]",
   "language": "python",
   "name": "conda-env-stylegan2-ada-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
